#PY_ENVIRONMENT=dev
PY_ENVIRONMENT=local # 启用本地开发环境
#PY_ENVIRONMENT=deploy
PY_DEBUG=true

# ---------注意-----------------------------------
# 如下模型中只能使用其中的某一个模型，不能同时配置多个模型
# 去对应的官网申请api-key,并替换YOUR API-KEY
# 也可以使用ollama本地运行的模型，api-key设置为ollama
# ⚠️文生图的模型暂时使用zhipuai，因此要配置zhipuai的api-key
# -----------------------------------------------

# 智普ai
LLM_BASE_URL=https://open.bigmodel.cn/api/paas/v4/chat/completions
LLM_API_SECRET_KEY=
LLM_API_KEY=
LLM_API_SECRET=
MODEL_NAME=glm-4

# kimi
#LLM_BASE_URL=https://api.moonshot.cn/v1
#LLM_API_KEY=YOUR API-KEY
#MODEL_NAME=moonshot-v1-8k

# 百川大模型
#LLM_BASE_URL=https://api.baichuan-ai.com/v1/
#LLM_API_KEY=YOUR API-KEY
#MODEL_NAME=Baichuan4

# 通义千问
#LLM_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1
#LLM_API_KEY=YOUR API-KEY
#MODEL_NAME=qwen-long

# 零一万物
#LLM_BASE_URL=https://api.lingyiwanwu.com/v1
#LLM_API_KEY=YOUR API-KEY
#MODEL_NAME=yi-large

# deepseek
# LLM_BASE_URL=https://api.deepseek.com
# LLM_API_KEY=ollama
# MODEL_NAME=deepseek-chat

# 豆包
#LLM_BASE_URL=https://ark.cn-beijing.volces.com/api/v3/
#LLM_API_KEY=YOUR API-KEY
# 注意：对于豆包api，model_name参数填入ENDPOINT_ID，具体申请操作在豆包api官网提供。
#MODEL_NAME=

# ollama
#LLM_BASE_URL=http://localhost:11434/v1/
#LLM_API_KEY=ollama
#MODEL_NAME=qwen2:0.5b

# anyapi
#LLM_BASE_URL=https://api.siliconflow.cn/v1
#LLM_API_KEY=YOUR API-KEY
#MODEL_NAME=Qwen/Qwen2-7B-Instruct

#文生图模型，暂时使用zhipuai
#OPENAI_API_KEY=YOUR API-KEY
ZHIPUAI_API_KEY=

# 这里填入你的组织名
ORGANIZATION_NAME= xxx团队

